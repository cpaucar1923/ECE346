{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "816729b5-ad6e-4131-934e-7d84d28a2f13",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lab 3: MDP and QMDP\n",
    "This lab aims to show you how to formulate a problem as an MDP (POMDP) and solve it using value iteration, policy iteration, and approximated solution for POMDP such as QMDP.\n",
    "\n",
    "## 1. MDP\n",
    "### Task 1.1: MDP formulation\n",
    "We provide you with a class `MDP()` to help you easily formulate your MDP problem. Refer to your lab handout for instructions on how to use the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "27599c2e-1945-4e5f-b275-716b66568bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "from mdp import MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a781589c-8a5a-4e9b-bdd5-26870272f629",
   "metadata": {},
   "source": [
    "Test out the class with the simple two-state problem in the handout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f364f75c-101a-4bde-b63b-fce52e49c25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoStateMDP(MDP):\n",
    "    def __init__(self):\n",
    "        self.states = [[\"s1\", \"s2\"]]\n",
    "        self.actions = [\"a0\", \"a1\"]\n",
    "        self.gam = 0.9\n",
    "    \n",
    "        # call the parent class\n",
    "        # notice that the state is a list of state variables\n",
    "        super().__init__(\n",
    "            states=self.states, actions=self.actions)\n",
    "        self.populate_data()\n",
    "    \n",
    "    def populate_data(self):\n",
    "        # add all routes from s1\n",
    "        self.add_route([\"s1\"],\"a0\",[\"s1\"])\n",
    "        self.add_route([\"s1\"],\"a1\",[\"s2\"])\n",
    "        # add all routes from s2\n",
    "        self.add_route([\"s2\"],\"a0\",[\"s2\"])\n",
    "        self.add_route([\"s2\"],\"a1\",[\"s2\"])\n",
    "        \n",
    "        # let's populate the reward, assuming r>0 is 0.5\n",
    "        for a in self.a:\n",
    "            self.add_reward([\"s1\"], a, 0.5)\n",
    "            self.add_reward([\"s2\"], a, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a1887459-28a7-49c9-894a-6cfe600b4540",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Everything is correct!\n"
     ]
    }
   ],
   "source": [
    "twoStateMDP = TwoStateMDP()\n",
    "assert twoStateMDP.get_index([\"s1\"]) == 0, \"Something is wrong\"\n",
    "assert twoStateMDP.get_state(0) == [0], \"Something is wrong\"\n",
    "assert twoStateMDP.get_real_state_value(0) == ['s1'], \"Something is wrong\"\n",
    "print(\"Everything is correct!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "703348b5-1d8b-4dd4-b025-8eb8cf89472d",
   "metadata": {},
   "source": [
    "Now let's use the `MDP()` class to formulate our T-intersection problem.\n",
    "\n",
    "**Task 1.1**: In `populate_data()`, all the probability value for each `self.add_route()` command is missing (denotes `MISSING_VALUE`). Compute the state transition matrix of the T-intersection MDP and fill in the missing `p` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d4c45cf1-92c5-4b0f-8adc-19787e2ebf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASK 1.1: Fill in MISSING_VALUE with correct p values\n",
    "class TIntersection(MDP):\n",
    "    def __init__(self, reward={\"forward\": -1.0, \"stop\": -5, \"collision\": -10, \"goal\": 5.0}):\n",
    "        self.states = [\n",
    "            [\"ego_{}\".format(x) for x in list(range(1, 6))],\n",
    "            [\"car_{}\".format(x) for x in list(range(1, 6))]\n",
    "        ]\n",
    "        self.actions = [\"forward\", \"stop\"]\n",
    "        self.gam = 0.9\n",
    "        super().__init__(\n",
    "            states=self.states, actions=self.actions, method=\"add\")\n",
    "        self.reward = reward\n",
    "        self.populate_data()\n",
    "    \n",
    "    def populate_data(self):\n",
    "        for i in range(1, 6):\n",
    "            for j in range(1, 6):\n",
    "                # add route for forward action\n",
    "                # i is our car\n",
    "                # j is the other car\n",
    "                # (i, j) --> (i+1, j): Our car moves forward, the other is stationary\n",
    "                self.add_route([\"ego_{}\".format(i), \"car_{}\".format(j)], \"forward\", [\"ego_{}\".format(min(i+1, 5)), \"car_{}\".format(j)], 0.64)\n",
    "                # (i, j) --> (i+1, j+1): Both cars move forward one time step\n",
    "                self.add_route([\"ego_{}\".format(i), \"car_{}\".format(j)], \"forward\", [\"ego_{}\".format(min(i+1, 5)), \"car_{}\".format(min(j+1, 5))], 0.128)\n",
    "                # (i, j) --> (i+1, j+2): Our car moves forward once, the other twice\n",
    "                self.add_route([\"ego_{}\".format(i), \"car_{}\".format(j)], \"forward\", [\"ego_{}\".format(min(i+1, 5)), \"car_{}\".format(min(j+2, 5))], 0.032)\n",
    "                # (i, j) --> (i+2, j): Our car moves forward twice, the other is stationary\n",
    "                self.add_route([\"ego_{}\".format(i), \"car_{}\".format(j)], \"forward\", [\"ego_{}\".format(min(i+2, 5)), \"car_{}\".format(j)], 0.16)\n",
    "                # (i, j) --> (i+2, j+1): Our car moves forward twice, the other forward once\n",
    "                self.add_route([\"ego_{}\".format(i), \"car_{}\".format(j)], \"forward\", [\"ego_{}\".format(min(i+2, 5)), \"car_{}\".format(min(j+1, 5))], 0.032)\n",
    "                # (i, j) --> (i+2, j+2): Both cars move forward twice\n",
    "                self.add_route([\"ego_{}\".format(i), \"car_{}\".format(j)], \"forward\", [\"ego_{}\".format(min(i+2, 5)), \"car_{}\".format(min(j+2, 5))], 0.008)\n",
    "\n",
    "                # add route for stop action\n",
    "                # (i, j) --> (i, j+2)\n",
    "                self.add_route([\"ego_{}\".format(i), \"car_{}\".format(j)], \"stop\", [\"ego_{}\".format(i), \"car_{}\".format(min(j+2, 5))], 0.08)\n",
    "                # (i, j) --> (i, j+1)\n",
    "                self.add_route([\"ego_{}\".format(i), \"car_{}\".format(j)], \"stop\", [\"ego_{}\".format(i), \"car_{}\".format(min(j+1, 5))], 0.32)\n",
    "                # (i, j) --> (i, j)\n",
    "                self.add_route([\"ego_{}\".format(i), \"car_{}\".format(j)], \"stop\", [\"ego_{}\".format(i), \"car_{}\".format(j)], 0.6)\n",
    "\n",
    "                self.add_reward([\"ego_{}\".format(i), \"car_{}\".format(j)], \"forward\", self.reward[\"forward\"])\n",
    "                self.add_reward([\"ego_{}\".format(i), \"car_{}\".format(j)], \"stop\", self.reward[\"stop\"])\n",
    "                        \n",
    "                # check for collision\n",
    "                if (i in [4, 5]) and (j in [4, 5]):\n",
    "                    self.add_reward([\"ego_{}\".format(i), \"car_{}\".format(j)], \"forward\", self.reward[\"collision\"])\n",
    "                    self.add_reward([\"ego_{}\".format(i), \"car_{}\".format(j)], \"stop\", self.reward[\"collision\"])\n",
    "                elif (i == 5) or (j == 5):\n",
    "                    # reaching goal\n",
    "                    self.add_reward([\"ego_{}\".format(i), \"car_{}\".format(j)], \"forward\", self.reward[\"goal\"])\n",
    "                    self.add_reward([\"ego_{}\".format(i), \"car_{}\".format(j)], \"stop\", self.reward[\"goal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ea65f5e7-9958-4399-9656-0e9d9bce091a",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "### TEST YOUR CODE\n",
    "tinter = TIntersection()\n",
    "init_state = [\"ego_1\", \"car_1\"]\n",
    "init_state_index = tinter.get_index(init_state)\n",
    "state_transition_matrix = tinter.P[:, 0, init_state_index]\n",
    "next_state_index = np.where(state_transition_matrix > 0.0)\n",
    "next_state_p = state_transition_matrix[next_state_index]\n",
    "assert np.array_equal(next_state_index[0], np.array([1, 2, 6, 7, 11, 12]))\n",
    "assert np.array_equal(next_state_p, np.array([0.64, 0.16, 0.128, 0.032, 0.032, 0.008]))\n",
    "\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e45a7cbc-3334-4b9b-bd50-6c561972703f",
   "metadata": {},
   "source": [
    "### Task 1.2: MDP value iteration and policy iteration\n",
    "Now we will write the value iteration function and policy iteration function or an arbitrary MDP that inherits the structure of our MDP class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e2220b35-921f-4646-a773-fec781aedf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASK 1.2: Write value iteration and policy iteration\n",
    "def value_iteration(threshold = .001, mdp:MDP=None):\n",
    "\n",
    "    if mdp is None:\n",
    "        raise ValueError(\"MDP cannot be None\")\n",
    "    numa, nums, R, P = mdp.get_mdp()\n",
    "    V_star = np.zeros(nums)\n",
    "    pi_star = np.zeros(nums)\n",
    "    print('R shape type:',R.shape )\n",
    "    print('P shape type:',P.shape )\n",
    "    Vk = np.zeros(nums) # initialize the value function to zero\n",
    "    # Vk = np.random.randint(low = 1, size=nums)\n",
    "    policy = np.zeros(nums, dtype=int)\n",
    "    Vk_plus_1 = np.zeros(nums)\n",
    "    while True:\n",
    "        Vk = Vk_plus_1.copy()\n",
    "        for s in range(nums):\n",
    "            Q = np.zeros(numa)\n",
    "            Q = R[s, :] + (mdp.gam * Vk @ P[:,s,:])\n",
    "            Vk_plus_1[s] = np.max(Q)\n",
    "            policy[s] = np.argmax(Q)\n",
    "        if np.linalg.norm(Vk_plus_1 - Vk) < threshold:\n",
    "            break\n",
    "\n",
    "    V_star = Vk_plus_1 # update the value function\n",
    "    pi_star = policy\n",
    "\n",
    "    return V_star, pi_star\n",
    "\n",
    "def policy_eval(policy, threshold = .001, mdp:MDP=None):\n",
    "    if mdp is None:\n",
    "        raise ValueError(\"MDP cannot be None\")\n",
    "    numa, nums, R, P = mdp.get_mdp()\n",
    "    Vk = np.zeros(nums)\n",
    "\n",
    "    # start with a random policy\n",
    "    identity = np.eye(nums)\n",
    "\n",
    "    p_pi = np.ones((nums,nums))\n",
    "    # r_pi = np.ones(nums)\n",
    "\n",
    "    for s in range(nums):\n",
    "        action = policy[s]\n",
    "        p_pi[s] = P[:,s,action]\n",
    "        Vk[s] = R[s,action]\n",
    "        \n",
    "    a = identity - mdp.gam * p_pi\n",
    "    b = Vk\n",
    "\n",
    "    V_star = np.linalg.solve(a,b)\n",
    "\n",
    "    return V_star\n",
    "    \n",
    "\n",
    "def policy_iteration(threshold = .001, mdp:MDP=None):\n",
    "    if mdp is None:\n",
    "        raise ValueError(\"MDP cannot be None\")\n",
    "    numa, nums, R, P = mdp.get_mdp()\n",
    "    # initialize a random policy with length nums and action randomly assigned from numa\n",
    "    pi_star = np.random.randint(0, numa, nums)\n",
    "    V_star = policy_eval(pi_star, mdp=mdp)\n",
    "    # print('Running before while loop in policy iteration')\n",
    "    while True:\n",
    "        # print('Running within while loop in policy iteration')\n",
    "        for s in range(nums):\n",
    "            old_action = pi_star[s]\n",
    "            # Q = np.zeros(numa)\n",
    "            Q = R[s,:] + (P[:,s,:] * mdp.gam * V_star)\n",
    "            pi_star[s] = np.argmax(Q)\n",
    "            if old_action != pi_star[s]:\n",
    "                policy_stable = False\n",
    "        if not policy_stable:\n",
    "            V_star = policy_eval(pi_star,threshold = threshold, mdp = mdp)\n",
    "            policy_stable = True\n",
    "        else:\n",
    "            break\n",
    "    return V_star, pi_star\n",
    "    ####\n",
    "    ## YOUR CODE HERE\n",
    "    # raise NotImplementedError(\"You have not written Policy Iteration\")\n",
    "    ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695ec1a3-ea3d-4348-ab62-909ba1fb3997",
   "metadata": {},
   "source": [
    "Let's test our value iteration and policy iteration on the `TwoStateMDP`. The following is the close-form value function for this simple MDP:\n",
    "\n",
    "$$\n",
    "V(s_1) = \\frac{r + \\gamma}{1 - \\gamma} \\;\\;,\\;\\; V(s_2) = \\frac{1 + r}{1 - \\gamma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "72248dd1-1c78-470c-81f5-4e1e244bf51e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R shape type: (2, 2)\nP shape type: (2, 2, 2)\nValue iteration is correct!\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 2 with size 2",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28mabs\u001b[39m(V_star \u001b[38;5;241m-\u001b[39m V_calc)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue iteration is incorrect\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue iteration is correct!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m V_star, pi_star_policy \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmdp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtwoStateMDP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28mabs\u001b[39m(V_star \u001b[38;5;241m-\u001b[39m V_calc0)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPolicy iteration is incorrect\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPolicy iteration is correct!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[74], line 73\u001b[0m, in \u001b[0;36mpolicy_iteration\u001b[0;34m(threshold, mdp)\u001b[0m\n\u001b[1;32m     71\u001b[0m         policy_stable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m policy_stable:\n\u001b[0;32m---> 73\u001b[0m     V_star \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpi_star\u001b[49m\u001b[43m,\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmdp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmdp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     policy_stable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[74], line 44\u001b[0m, in \u001b[0;36mpolicy_eval\u001b[0;34m(policy, threshold, mdp)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nums):\n\u001b[1;32m     43\u001b[0m     action \u001b[38;5;241m=\u001b[39m policy[s]\n\u001b[0;32m---> 44\u001b[0m     p_pi[s] \u001b[38;5;241m=\u001b[39m \u001b[43mP\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     45\u001b[0m     Vk[s] \u001b[38;5;241m=\u001b[39m R[s,action]\n\u001b[1;32m     47\u001b[0m a \u001b[38;5;241m=\u001b[39m identity \u001b[38;5;241m-\u001b[39m mdp\u001b[38;5;241m.\u001b[39mgam \u001b[38;5;241m*\u001b[39m p_pi\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 2 with size 2"
     ]
    }
   ],
   "source": [
    "### TEST YOUR CODE\n",
    "V_star, pi_star_value = value_iteration(mdp=twoStateMDP, threshold=1e-10)\n",
    "r = 0.5\n",
    "V_calc = [(r + twoStateMDP.gam)/(1-twoStateMDP.gam), (1 + r)/(1-twoStateMDP.gam)]\n",
    "assert np.max(abs(V_star - V_calc)) < 1e-2, \"Value iteration is incorrect\"\n",
    "print(\"Value iteration is correct!\")\n",
    "\n",
    "V_star, pi_star_policy = policy_iteration(mdp=twoStateMDP, threshold=1e-10)\n",
    "assert np.max(abs(V_star - V_calc0)) < 1e-2, \"Policy iteration is incorrect\"\n",
    "print(\"Policy iteration is correct!\")\n",
    "\n",
    "assert np.array_equal(pi_star_policy, pi_star_value), \"Policy and value iteration give different pi star\"\n",
    "print(\"Policy learned by value iteration: {}\".format(pi_star_value))\n",
    "print(\"Policy learned by policy iteration: {}\".format(pi_star_policy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ae34cf-f9d2-43b6-b727-6ca0829b7d04",
   "metadata": {},
   "source": [
    "Let's now test the value iteration and policy iteration on our T-intersection MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([14., 15.])"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "V_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b8c3a903-8c39-4e98-9086-655d3c666b28",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R shape type: (25, 2)\nP shape type: (25, 25, 2)\nV*: [ -4.44751642  -5.8650576   -6.86963398  -3.82067653   2.17932347\n   4.08990375   2.72621509   2.72621509 -24.87752784 -18.87752784\n  15.41775778  14.65027196  14.65027196 -52.42854825 -46.42854825\n  28.3478145   28.26085797  28.26085797 -99.99997682 -99.99997682\n  49.99998841  49.99998841  49.99998841 -99.99997682 -99.99997682]\npi*: [0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "V_star, pi_star = value_iteration(mdp=tinter, threshold=1e-5)\n",
    "print(\"V*: {}\".format(V_star))\n",
    "print(\"pi*: {}\".format(pi_star))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7df012f7",
   "metadata": {},
   "source": [
    "### Task 1.3: Simulate your computed $\\pi^*$\n",
    "\n",
    "We provide you with a class `TintersectionVisualizer()` to visualize your MDP. Simply call the following function to plot the current state:\n",
    "```python\n",
    "# initialize the visualizer\n",
    "vis = TIntersectionVisualizer()\n",
    "# define the state\n",
    "state = [\"ego_1\", \"car_1\"]\n",
    "# visualize the state\n",
    "vis.plot(state)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "805792f3",
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 666.667x500 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"386.4pt\" height=\"291.6pt\" viewBox=\"0 0 386.4 291.6\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-04-15T11:55:28.023837</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.6.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 291.6 \nL 386.4 291.6 \nL 386.4 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"line2d_1\">\n    <path d=\"M 44.4 292.6 \nL 44.4 -1 \n\" clip-path=\"url(#p77c3d099e2)\" style=\"fill: none; stroke: #000000; stroke-width: 2; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_2\">\n    <path d=\"M 182.04 292.6 \nL 182.04 192 \n\" clip-path=\"url(#p77c3d099e2)\" style=\"fill: none; stroke: #000000; stroke-width: 2; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_3\">\n    <path d=\"M 182.04 55.248 \nL 182.04 -1 \n\" clip-path=\"url(#p77c3d099e2)\" style=\"fill: none; stroke: #000000; stroke-width: 2; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_4\">\n    <path d=\"M 44.4 192 \nL 44.4 192 \n\" clip-path=\"url(#p77c3d099e2)\" style=\"fill: none; stroke: #000000; stroke-width: 2; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_5\">\n    <path d=\"M 182.04 192 \nL 387.4 192 \n\" clip-path=\"url(#p77c3d099e2)\" style=\"fill: none; stroke: #000000; stroke-width: 2; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_6\">\n    <path d=\"M 44.4 55.248 \nL 44.4 55.248 \n\" clip-path=\"url(#p77c3d099e2)\" style=\"fill: none; stroke: #000000; stroke-width: 2; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_7\">\n    <path d=\"M 182.04 55.248 \nL 387.4 55.248 \n\" clip-path=\"url(#p77c3d099e2)\" style=\"fill: none; stroke: #000000; stroke-width: 2; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_8\">\n    <path d=\"M 113.22 292.6 \nL 113.22 -1 \n\" clip-path=\"url(#p77c3d099e2)\" style=\"fill: none; stroke-dasharray: 10,10; stroke-dashoffset: 0; stroke: #000000; stroke-width: 2\"/>\n   </g>\n   <g id=\"line2d_9\">\n    <path d=\"M 182.04 123.624 \nL 387.4 123.624 \n\" clip-path=\"url(#p77c3d099e2)\" style=\"fill: none; stroke-dasharray: 10,10; stroke-dashoffset: 0; stroke: #000000; stroke-width: 2\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <defs>\n     <path id=\"me4b543257d\" d=\"M 0 3.535534 \nC 0.937635 3.535534 1.836992 3.163008 2.5 2.5 \nC 3.163008 1.836992 3.535534 0.937635 3.535534 0 \nC 3.535534 -0.937635 3.163008 -1.836992 2.5 -2.5 \nC 1.836992 -3.163008 0.937635 -3.535534 0 -3.535534 \nC -0.937635 -3.535534 -1.836992 -3.163008 -2.5 -2.5 \nC -3.163008 -1.836992 -3.535534 -0.937635 -3.535534 0 \nC -3.535534 0.937635 -3.163008 1.836992 -2.5 2.5 \nC -1.836992 3.163008 -0.937635 3.535534 0 3.535534 \nz\n\" style=\"stroke: #ff0000; stroke-opacity: 0.5\"/>\n    </defs>\n    <g clip-path=\"url(#p77c3d099e2)\">\n     <use xlink:href=\"#me4b543257d\" x=\"78.81\" y=\"44.16\" style=\"fill: #ff0000; fill-opacity: 0.5; stroke: #ff0000; stroke-opacity: 0.5\"/>\n    </g>\n   </g>\n   <g id=\"PathCollection_2\">\n    <g clip-path=\"url(#p77c3d099e2)\">\n     <use xlink:href=\"#me4b543257d\" x=\"83.842912\" y=\"106.648026\" style=\"fill: #ff0000; fill-opacity: 0.5; stroke: #ff0000; stroke-opacity: 0.5\"/>\n    </g>\n   </g>\n   <g id=\"PathCollection_3\">\n    <g clip-path=\"url(#p77c3d099e2)\">\n     <use xlink:href=\"#me4b543257d\" x=\"113.902\" y=\"135.396992\" style=\"fill: #ff0000; fill-opacity: 0.5; stroke: #ff0000; stroke-opacity: 0.5\"/>\n    </g>\n   </g>\n   <g id=\"PathCollection_4\">\n    <g clip-path=\"url(#p77c3d099e2)\">\n     <use xlink:href=\"#me4b543257d\" x=\"198.443216\" y=\"153.936621\" style=\"fill: #ff0000; fill-opacity: 0.5; stroke: #ff0000; stroke-opacity: 0.5\"/>\n    </g>\n   </g>\n   <g id=\"PathCollection_5\">\n    <g clip-path=\"url(#p77c3d099e2)\">\n     <use xlink:href=\"#me4b543257d\" x=\"323.316672\" y=\"160.922554\" style=\"fill: #ff0000; fill-opacity: 0.5; stroke: #ff0000; stroke-opacity: 0.5\"/>\n    </g>\n   </g>\n   <g id=\"PathCollection_6\">\n    <defs>\n     <path id=\"me3f8f40f32\" d=\"M 0 7.071068 \nC 1.875269 7.071068 3.673985 6.326016 5 5 \nC 6.326016 3.673985 7.071068 1.875269 7.071068 0 \nC 7.071068 -1.875269 6.326016 -3.673985 5 -5 \nC 3.673985 -6.326016 1.875269 -7.071068 0 -7.071068 \nC -1.875269 -7.071068 -3.673985 -6.326016 -5 -5 \nC -6.326016 -3.673985 -7.071068 -1.875269 -7.071068 0 \nC -7.071068 1.875269 -6.326016 3.673985 -5 5 \nC -3.673985 6.326016 -1.875269 7.071068 0 7.071068 \nz\n\" style=\"stroke: #ff0000\"/>\n    </defs>\n    <g clip-path=\"url(#p77c3d099e2)\">\n     <use xlink:href=\"#me3f8f40f32\" x=\"78.81\" y=\"44.16\" style=\"fill: #ff0000; stroke: #ff0000\"/>\n    </g>\n   </g>\n   <g id=\"PathCollection_7\">\n    <defs>\n     <path id=\"m95209cf5d1\" d=\"M 0 3.535534 \nC 0.937635 3.535534 1.836992 3.163008 2.5 2.5 \nC 3.163008 1.836992 3.535534 0.937635 3.535534 0 \nC 3.535534 -0.937635 3.163008 -1.836992 2.5 -2.5 \nC 1.836992 -3.163008 0.937635 -3.535534 0 -3.535534 \nC -0.937635 -3.535534 -1.836992 -3.163008 -2.5 -2.5 \nC -3.163008 -1.836992 -3.535534 -0.937635 -3.535534 0 \nC -3.535534 0.937635 -3.163008 1.836992 -2.5 2.5 \nC -1.836992 3.163008 -0.937635 3.535534 0 3.535534 \nz\n\" style=\"stroke: #0000ff; stroke-opacity: 0.5\"/>\n    </defs>\n    <g clip-path=\"url(#p77c3d099e2)\">\n     <use xlink:href=\"#m95209cf5d1\" x=\"145.77\" y=\"265.92\" style=\"fill: #0000ff; fill-opacity: 0.5; stroke: #0000ff; stroke-opacity: 0.5\"/>\n    </g>\n   </g>\n   <g id=\"PathCollection_8\">\n    <g clip-path=\"url(#p77c3d099e2)\">\n     <use xlink:href=\"#m95209cf5d1\" x=\"146.987184\" y=\"221.353139\" style=\"fill: #0000ff; fill-opacity: 0.5; stroke: #0000ff; stroke-opacity: 0.5\"/>\n    </g>\n   </g>\n   <g id=\"PathCollection_9\">\n    <g clip-path=\"url(#p77c3d099e2)\">\n     <use xlink:href=\"#m95209cf5d1\" x=\"156.93\" y=\"178.881664\" style=\"fill: #0000ff; fill-opacity: 0.5; stroke: #0000ff; stroke-opacity: 0.5\"/>\n    </g>\n   </g>\n   <g id=\"PathCollection_10\">\n    <g clip-path=\"url(#p77c3d099e2)\">\n     <use xlink:href=\"#m95209cf5d1\" x=\"208.487712\" y=\"158.384634\" style=\"fill: #0000ff; fill-opacity: 0.5; stroke: #0000ff; stroke-opacity: 0.5\"/>\n    </g>\n   </g>\n   <g id=\"PathCollection_11\">\n    <g clip-path=\"url(#p77c3d099e2)\">\n     <use xlink:href=\"#m95209cf5d1\" x=\"319.171104\" y=\"158.356051\" style=\"fill: #0000ff; fill-opacity: 0.5; stroke: #0000ff; stroke-opacity: 0.5\"/>\n    </g>\n   </g>\n   <g id=\"PathCollection_12\">\n    <defs>\n     <path id=\"m3af52182e8\" d=\"M 0 7.071068 \nC 1.875269 7.071068 3.673985 6.326016 5 5 \nC 6.326016 3.673985 7.071068 1.875269 7.071068 0 \nC 7.071068 -1.875269 6.326016 -3.673985 5 -5 \nC 3.673985 -6.326016 1.875269 -7.071068 0 -7.071068 \nC -1.875269 -7.071068 -3.673985 -6.326016 -5 -5 \nC -6.326016 -3.673985 -7.071068 -1.875269 -7.071068 0 \nC -7.071068 1.875269 -6.326016 3.673985 -5 5 \nC -3.673985 6.326016 -1.875269 7.071068 0 7.071068 \nz\n\" style=\"stroke: #0000ff\"/>\n    </defs>\n    <g clip-path=\"url(#p77c3d099e2)\">\n     <use xlink:href=\"#m3af52182e8\" x=\"145.77\" y=\"265.92\" style=\"fill: #0000ff; stroke: #0000ff\"/>\n    </g>\n   </g>\n   <g id=\"line2d_10\">\n    <path d=\"M 78.81 44.16 \nL 78.52527 53.187721 \nL 78.390303 61.506408 \nL 78.417278 69.157 \nL 78.618375 76.180439 \nL 79.005775 82.617665 \nL 79.591656 88.509619 \nL 80.388198 93.897241 \nL 81.407582 98.821472 \nL 82.661988 103.323253 \nL 84.163594 107.443524 \nL 85.924581 111.223227 \nL 87.957129 114.703301 \nL 90.273418 117.924687 \nL 92.885627 120.928327 \nL 95.805936 123.75516 \nL 99.046525 126.446127 \nL 102.622542 129.040092 \nL 106.583638 131.551765 \nL 111.001724 133.980275 \nL 115.949082 136.324491 \nL 121.497994 138.583282 \nL 127.720743 140.755517 \nL 134.68961 142.840065 \nL 142.476878 144.835795 \nL 151.151314 146.741477 \nL 160.700852 148.553606 \nL 171.032593 150.266399 \nL 182.050122 151.873976 \nL 193.657026 153.370454 \nL 205.756891 154.749954 \nL 218.253301 156.006594 \nL 231.049844 157.134494 \nL 244.050104 158.127772 \nL 257.157668 158.980547 \nL 270.276122 159.686939 \nL 283.309052 160.241066 \nL 296.160043 160.637048 \nL 308.732682 160.869003 \nL 320.930554 160.93105 \nL 332.657245 160.81731 \nL 343.816341 160.521899 \nL 354.311429 160.038939 \nL 364.046093 159.362547 \nL 372.923919 158.486842 \nL 380.848495 157.405944 \nL 387.4 156.174748 \n\" clip-path=\"url(#p77c3d099e2)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff0000; stroke-opacity: 0.5; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_11\">\n    <path d=\"M 145.77 265.92 \nL 146.046762 261.62006 \nL 146.237355 257.231774 \nL 146.361585 252.767469 \nL 146.439258 248.239472 \nL 146.490181 243.66011 \nL 146.534161 239.041712 \nL 146.591003 234.396604 \nL 146.680514 229.737114 \nL 146.822502 225.07557 \nL 147.036771 220.424298 \nL 147.343129 215.795627 \nL 147.761382 211.201884 \nL 148.311337 206.655395 \nL 149.012799 202.16849 \nL 149.885575 197.753494 \nL 150.949473 193.422736 \nL 152.226524 189.189917 \nL 153.764637 185.084709 \nL 155.628419 181.14709 \nL 157.882752 177.41721 \nL 160.592519 173.935216 \nL 163.822604 170.741259 \nL 167.637889 167.875486 \nL 172.103258 165.378048 \nL 177.281759 163.28741 \nL 183.194222 161.603346 \nL 189.819261 160.286938 \nL 197.133656 159.297584 \nL 205.114185 158.594682 \nL 213.737626 158.137632 \nL 222.980758 157.885832 \nL 232.82036 157.79868 \nL 243.233211 157.835575 \nL 254.196089 157.955916 \nL 265.685773 158.119102 \nL 277.679041 158.284531 \nL 290.152672 158.411601 \nL 303.083445 158.459712 \nL 316.448138 158.388262 \nL 330.223531 158.156649 \nL 344.386401 157.724272 \nL 358.913528 157.050531 \nL 373.781689 156.094822 \nL 387.4 154.948504 \n\" clip-path=\"url(#p77c3d099e2)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #0000ff; stroke-opacity: 0.5; stroke-width: 1.5\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p77c3d099e2\">\n   <rect x=\"7.2\" y=\"7.2\" width=\"372\" height=\"277.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "from visualizer import TIntersectionVisualizer\n",
    "vis = TIntersectionVisualizer()\n",
    "state = [\"ego_1\", \"car_1\"]\n",
    "vis.plot(state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98e1ead6",
   "metadata": {},
   "source": [
    "Do the following task:\n",
    "1. Choose an initial state\n",
    "2. From the computed $V^*(x), \\pi^*(x)$, iterate from the chosen initial state gradually until you reach the terminal state, i.e. `[\"ego_5\", \"car_5\"]`.\n",
    "3. While iterating, keep track of your state-action pair $(s, a)$. Print out all the state-action pairs that your computed $V^*(x), \\pi^*(x)$ navigate you to.\n",
    "\n",
    "*Hint 1*: For each state and action pair $(s, a)$, the matrix P will show you the probability of $s'$ by calling `P[:, state_index, action_index]`\n",
    "\n",
    "*Hint 2*: Assuming that you have a list of states `S` and the probability distribution `p` of the states in `S`, you can use `numpy.random.choice(S, p=p)` to sample the next state, given `S` and `p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "82336901",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n",
      "next state p [0.64  0.16  0.128 0.032 0.032 0.008]\n",
      "(6,)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state_list\n\u001b[1;32m     33\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mego_1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcar_1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m# choose an initial condition\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m state_list \u001b[38;5;241m=\u001b[39m \u001b[43mget_solution\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 20\u001b[0m, in \u001b[0;36mget_solution\u001b[0;34m(initial_state)\u001b[0m\n\u001b[1;32m     18\u001b[0m next_state_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(state_transition_matrix \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m     19\u001b[0m next_state_p \u001b[38;5;241m=\u001b[39m state_transition_matrix[next_state_index[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext state p\u001b[39m\u001b[38;5;124m'\u001b[39m, next_state_p)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(next_state_p\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 4. Sample next state index, given the transition probabilities\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "def get_solution(initial_state):\n",
    "    done = False\n",
    "    state = initial_state \n",
    "    state_list = [state]\n",
    "    print(\"State: {}\\tAction: {}\".format(state, None))\n",
    "    numa, nums, R, P = tinter.get_mdp()\n",
    "    \n",
    "    \n",
    "    while not done:\n",
    "        ## YOUR CODE HERE\n",
    "        # 1. Get index of current state\n",
    "        curr_index = tinter.get_index(state)\n",
    "        # 2. Get next action based on pi_star and state index\n",
    "        action = pi_star[curr_index]\n",
    "        # 3. Get indices of all the possible next states and their transition probabilities\n",
    "        state_transition_matrix = P[:,curr_index, action]\n",
    "        next_state_index = np.where(state_transition_matrix > 0.0)\n",
    "        next_state_p = state_transition_matrix[next_state_index[0]]\n",
    "        print('next state p', next_state_p)\n",
    "        print(next_state_p.shape)\n",
    "        \n",
    "        # 4. Sample next state index, given the transition probabilities\n",
    "        next_state = np.random.choice(next_state_index[0], size = 1, p = next_state_p)\n",
    "\n",
    "        # 5. Append new state (readable form, e.g. [\"ego_1\", \"car_2\"]) to state_list\n",
    "        state_list.append(tinter.get_real_state_value(next_state))\n",
    "\n",
    "        if state[0] == \"ego_5\" or state[1] == \"car_5\":\n",
    "            done = True\n",
    "\n",
    "    return state_list\n",
    "initial_state = [\"ego_1\", \"car_1\"] # choose an initial condition\n",
    "state_list = get_solution(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce6c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from IPython.display import Image\n",
    "\n",
    "folder = \"figures\"\n",
    "sub_folder = \"mdp\"\n",
    "\n",
    "fig_prog_folder = os.path.join(folder, sub_folder)\n",
    "if os.path.exists(fig_prog_folder):\n",
    "    print(\"WARNING: Path {} exists, GIF result might be affected with old data\".format(fig_prog_folder))\n",
    "os.makedirs(fig_prog_folder, exist_ok=True)\n",
    "\n",
    "for i, state in enumerate(state_list):\n",
    "    vis.plot(state)\n",
    "    plt.savefig(os.path.join(fig_prog_folder, \"{}.png\".format(i)), dpi=200)\n",
    "    plt.clf()\n",
    "\n",
    "gif_path = os.path.join(fig_prog_folder, 'result.gif')\n",
    "length = len([i for i in os.listdir(os.path.join(fig_prog_folder)) if \".png\" in i])\n",
    "\n",
    "with imageio.get_writer(gif_path, mode='I') as writer:\n",
    "    for i in range(length):\n",
    "        print(i, end='\\r')\n",
    "        filename = os.path.join(fig_prog_folder, str(i)+\".png\")\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "Image(open(gif_path,'rb').read(), width=400)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6c89ab7",
   "metadata": {},
   "source": [
    "Let's play around with this a bit. Let's change the default reward function of our MDP.\n",
    "\n",
    "The class `TIntersection()` takes in a dictionary with reward information of the following structure:\n",
    "\n",
    "```python\n",
    "TIntersection(reward = {\n",
    "    \"forward\": forward_r,\n",
    "    \"stop\": stop_r,\n",
    "    \"collision\": collision_r,\n",
    "    \"goal\": goal_r\n",
    "})\n",
    "```\n",
    "\n",
    "Let's change the reward function and see how it affects the $\\pi^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210cf7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tinter = TIntersection(reward={\n",
    "    \"forward\": 0.0,\n",
    "    \"stop\": -1.0,\n",
    "    \"collision\": -100,\n",
    "    \"goal\": 20\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4305d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_star, pi_star = value_iteration(mdp=tinter, threshold=1e-5)\n",
    "print(\"V*: {}\".format(V_star))\n",
    "print(\"pi*: {}\".format(pi_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab503931",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = ... # choose an initial state\n",
    "get_solution(initial_state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffd29442",
   "metadata": {},
   "source": [
    "With this new reward function, it is not good almost everywhere to choose the action 0, i.e. forward."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b120a2ff",
   "metadata": {},
   "source": [
    "## 2. QMDP\n",
    "### Task 2.1: Defining new MDP\n",
    "Assume that in this new problem, we do not know exactly where the other car is. Let's introduce a new action called *look* into our problem formulation. Everytime we choose the action *look*, no car moves, we get a $-1$ reward, and receives the observation $z \\in \\{1 \\dots 5\\}$ corresponding to the position of the other car.\n",
    "\n",
    "The first thing we will do to approximate this POMDP with QMDP is to solve for the underlying MDP formulation (assuming that all states are fully observable).\n",
    "\n",
    "Tasks:\n",
    "1. Edit the class `TIntersection()` to include the new information mentioned\n",
    "2. Use value iteration/policy iteration and solve for $\\hat{V}^{\\text{MDP}}$\n",
    "\n",
    "**NOTE**: You can copy the value of all the missing probability values from the previous questions (denotes `MISSING_VALUE`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1a90ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TIntersectionQMDP(MDP):\n",
    "    def __init__(self, reward={\"forward\": -1, \"stop\": -5, \"collision\": -10, \"goal\": 5.0, \"look\": -1}):\n",
    "        self.states = [\n",
    "            [\"ego_{}\".format(x) for x in list(range(1, 6))],\n",
    "            [\"car_{}\".format(x) for x in list(range(1, 6))]\n",
    "        ]\n",
    "        \n",
    "        # 1. Add another action \"look\" to self.actions\n",
    "        self.actions = [\"forward\", \"stop\", \"look\"]\n",
    "\n",
    "        self.gam = 0.9\n",
    "        super().__init__(\n",
    "            states=self.states, actions=self.actions, method=\"add\")\n",
    "        self.reward = reward\n",
    "        self.populate_data()\n",
    "        \n",
    "    def populate_data(self):\n",
    "        for i in range(1, 6):\n",
    "            for j in range(1, 6):\n",
    "                # add route for forward action\n",
    "                # (i, j) --> (i+1, j)\n",
    "                self.add_route([\"ego_{}\".format(i), \"car_{}\".format(j)], \"forward\", [\"ego_{}\".format(min(i+1, 5)), \"car_{}\".format(j)], 0.64)\n",
    "                # (i, j) --> (i+1, j+1)\n",
    "                self.add_route([\"ego_{}\".format(i), \"car_{}\".format(j)], \"forward\", [\"ego_{}\".format(min(i+1, 5)), \"car_{}\".format(min(j+1, 5))], 0.128)\n",
    "                # (i, j) --> (i+1, j+2)\n",
    "                self.add_route([\"ego_{}\".format(i), \"car_{}\".format(j)], \"forward\", [\"ego_{}\".format(min(i+1, 5)), \"car_{}\".format(min(j+2, 5))], 0.032)\n",
    "                # (i, j) --> (i+2, j)\n",
    "                self.add_route([\"ego_{}\".format(i), \"car_{}\".format(j)], \"forward\", [\"ego_{}\".format(min(i+2, 5)), \"car_{}\".format(j)], 0.16)\n",
    "                # (i, j) --> (i+2, j+1)\n",
    "                self.add_route([\"ego_{}\".format(i), \"car_{}\".format(j)], \"forward\", [\"ego_{}\".format(min(i+2, 5)), \"car_{}\".format(min(j+1, 5))], 0.032)\n",
    "                # (i, j) --> (i+2, j+2)\n",
    "                self.add_route([\"ego_{}\".format(i), \"car_{}\".format(j)], \"forward\", [\"ego_{}\".format(min(i+2, 5)), \"car_{}\".format(min(j+2, 5))], 0.008)\n",
    "\n",
    "                # add route for stop action\n",
    "                # (i, j) --> (i+2, j)\n",
    "                self.add_route([\"ego_{}\".format(i), \"car_{}\".format(j)], \"stop\", [\"ego_{}\".format(i), \"car_{}\".format(min(j+2, 5))], 0.008)\n",
    "                # (i, j) --> (i+1, j)\n",
    "                self.add_route([\"ego_{}\".format(i), \"car_{}\".format(j)], \"stop\", [\"ego_{}\".format(i), \"car_{}\".format(min(j+1, 5))], 0.32)\n",
    "                # (i, j) --> (i, j)\n",
    "                self.add_route([\"ego_{}\".format(i), \"car_{}\".format(j)], \"stop\", [\"ego_{}\".format(i), \"car_{}\".format(j)], 0.6)\n",
    "\n",
    "                self.add_reward([\"ego_{}\".format(i), \"car_{}\".format(j)], \"forward\", self.reward[\"forward\"])\n",
    "                self.add_reward([\"ego_{}\".format(i), \"car_{}\".format(j)], \"stop\", self.reward[\"stop\"])\n",
    "\n",
    "                # add route and reward for look action\n",
    "                ## YOUR CODE HERE\n",
    "                self.add_route([\"ego_{}\".format(i), \"car_{}\".format(j)], \"look\",[\"ego_{}\".format(i), \"car_{}\".format(j)] , p=1)\n",
    "                # ...\n",
    "                self.add_reward([\"ego_{}\".format(i), \"car_{}\".format(j)], \"look\", self.reward[\"look\"])\n",
    "                ######\n",
    "                        \n",
    "                # check for collision\n",
    "                if (i in [4, 5]) and (j in [4, 5]):\n",
    "                    self.add_reward([\"ego_{}\".format(i), \"car_{}\".format(j)], \"forward\", self.reward[\"collision\"])\n",
    "                    self.add_reward([\"ego_{}\".format(i), \"car_{}\".format(j)], \"stop\", self.reward[\"collision\"])\n",
    "                    self.add_reward([\"ego_{}\".format(i), \"car_{}\".format(j)], \"look\", self.reward[\"collision\"])\n",
    "                elif (i == 5) or (j == 5):\n",
    "                    # reaching goal\n",
    "                    self.add_reward([\"ego_{}\".format(i), \"car_{}\".format(j)], \"forward\", self.reward[\"goal\"])\n",
    "                    self.add_reward([\"ego_{}\".format(i), \"car_{}\".format(j)], \"stop\", self.reward[\"goal\"])\n",
    "                    self.add_reward([\"ego_{}\".format(i), \"car_{}\".format(j)], \"look\", self.reward[\"goal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9d7dd02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tinter_qmdp = TIntersectionQMDP()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6917f9ec",
   "metadata": {},
   "source": [
    "### Task 2.2: QMDP\n",
    "Write the QMDP function to get the **next action**, taking in consideration the belief space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "407fee3e",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R shape type: (25, 3)\nP shape type: (25, 25, 3)\n"
     ]
    }
   ],
   "source": [
    "# calculate the V_star using previously built value iteration\n",
    "V_star, pi_star = value_iteration(mdp=tinter_qmdp, threshold=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c0e810c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QMDP(V_star, belief, mdp:MDP=None):\n",
    "    if mdp is None:\n",
    "        raise ValueError(\"MDP cannot be None\")\n",
    "    \n",
    "    numa, nums, R, P = mdp.get_mdp()\n",
    "\n",
    "    # compute MDP-value for state-action pairs (Q)\n",
    "    ####\n",
    "    Q = np.zeros((numa, nums))\n",
    "    for a in range(numa):\n",
    "        Q [a, :] = R[:,a]+V_star @ P[:,:,a]\n",
    "    return np.argmax(np.sum(belief * Q,axis=1))\n",
    "    \n",
    "\n",
    "\n",
    "    ## YOUR CODE HERE\n",
    "    # raise NotImplementedError(\"Your QMDP function is empty\")\n",
    "\n",
    "    ####\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "488719ca",
   "metadata": {},
   "source": [
    "As we know exactly where we are all the time, and we only do not know where the other car is. Let us keep track of the probability distribution of where the other car is using an array of `b = [p_1, p_2, p_3, p_4, p_5]`, with `p_i` indicates the probability that the other car is at the $i^\\text{th}$ position, $i \\in \\{1 \\dots 5\\}$.\n",
    "\n",
    "We then provide you with the function `propagate_belief()` to incorporate our current position to give you the full array of belief space $\\in \\mathbb{R}^{25}$\n",
    "\n",
    "**Example**: Let us have our current belief `b = [0.2, 0.2, 0.2, 0.2, 0.2]` and that we are at `ego_1`, running `propagate_belief(b, \"ego_1\")` gives us:\n",
    "\n",
    "```python\n",
    "propagate_belief(b, \"ego_1\")\n",
    ">>> [0.2 0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.2 0.  0.\n",
    " 0.  0.  0.2 0.  0.  0.  0. ]\n",
    "```\n",
    "\n",
    "This is because the state array is in the form `[(ego_1, car_1), (ego_1, car_2), ..., (ego_2, car_1), (ego_2, car_2), ..., (ego_5, car_5)]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "00fe6326",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.2 0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.2 0.  0.  0.  0.  0.2 0.  0.\n 0.  0.  0.2 0.  0.  0.  0. ]\n"
     ]
    }
   ],
   "source": [
    "# assume that the initial belief space is {0.2, 0.2, 0.2, 0.2, 0.2} for the 5 positions that the other car can be in\n",
    "# if *look* is chosen, we will receive the observation of where the other car is, with probability distribution \n",
    "# p(pos-1) = 0.1, p(pos)=0.8, p(pos+1)=0.1 with pos is the true position\n",
    "b = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "\n",
    "# write a function to propagate the belief space to include our position as well\n",
    "def propagate_belief(b_car, ego_state):\n",
    "    idx = int(ego_state.replace(\"ego_\", \"\")) - 1\n",
    "    b_full = np.zeros(25)\n",
    "    for i, p in enumerate(b_car):\n",
    "        b_full[i*5 + idx] = p\n",
    "    return b_full\n",
    "\n",
    "print(propagate_belief(b, \"ego_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d568c7fd",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best next action: 0\nBest next action: 0\nBest next action: 2\n"
     ]
    }
   ],
   "source": [
    "## TEST YOUR CODE\n",
    "# we know exactly where we are, and we are confident that the other car is at position 1\n",
    "belief = propagate_belief([1.0, 0.0, 0.0, 0.0, 0.0], \"ego_1\")\n",
    "next_action = QMDP(V_star, belief, mdp=tinter_qmdp)\n",
    "print(\"Best next action: {}\".format(next_action))\n",
    "\n",
    "# we know exactly where we are, and we are confident that the other car is at position 3\n",
    "belief = propagate_belief([0.0, 0.0, 1.0, 0.0, 0.0], \"ego_3\")\n",
    "next_action = QMDP(V_star, belief, mdp=tinter_qmdp)\n",
    "print(\"Best next action: {}\".format(next_action))\n",
    "\n",
    "# we know exactly where we are, and we are not that confident where the other is, with high prop that\n",
    "# it's around position 2 or 3\n",
    "belief = propagate_belief([0.0, 0.4, 0.5, 0.1, 0.0], \"ego_3\")\n",
    "next_action = QMDP(V_star, belief, mdp=tinter_qmdp)\n",
    "print(\"Best next action: {}\".format(next_action))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9aaa673",
   "metadata": {},
   "source": [
    "### Task 2.3: Observation and belief space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03578732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume that the initial belief space is {0.2, 0.2, 0.2, 0.2, 0.2} for the 5 positions that the other car can be in\n",
    "# if *look* is chosen, we will receive the observation of where the other car is, with probability distribution \n",
    "# p(pos-1) = 0.1, p(pos)=0.8, p(pos+1)=0.1 with pos is the true position\n",
    "b = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "\n",
    "# write a function to propagate the belief space to include our position as well\n",
    "def propagate_belief(b_car, ego_state):\n",
    "    idx = int(ego_state.replace(\"ego_\", \"\")) - 1\n",
    "    b_full = np.zeros(25)\n",
    "    for i, p in enumerate(b_car):\n",
    "        b_full[i*5 + idx] = p\n",
    "    return b_full"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccd01e13",
   "metadata": {},
   "source": [
    "We will use Bayesian inference to update our belief using the observation, and we will model our belief space as a multinomial distribution, so that we can easily do Bayesian inference with conjugate prior. In this case, it will be Dirichlet distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7238f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirichletMultinominal:\n",
    "    def __init__(self, alpha=[1, 1, 1, 1, 1]):\n",
    "        self.alpha = alpha\n",
    "        self.observations = []\n",
    "    \n",
    "    def reset(self):\n",
    "        self.observations = []\n",
    "    \n",
    "    def get_posterior_predictive(self, observations=[]):\n",
    "        ### YOUR CODE HERE\n",
    "        # 1. Append the new observations to the internal observation list\n",
    "        self.observations.append(observations)\n",
    "\n",
    "        # 2. Calculate the predictive posterior distribution and return\n",
    "        # raise NotImplementedError\n",
    "        pos_dist = []\n",
    "        for a in range(len(self.alpha)):\n",
    "            numerator = self.alpha[a] + observations.count(self.alpha[a])\n",
    "            denominator = np.sum(self.alpha) + len(observations)\n",
    "            print('numerator is ', numerator)\n",
    "            print('denominator is', denominator)\n",
    "            pos_dist.append(numerator / denominator)\n",
    "        print('distribution is:',pos_dist)\n",
    "            # pos_dist[a] = (self.alpha[a] + observations.count(self.alpha[a]))/ (np.sum(self.alpha) + len(observations))\n",
    "        ###\n",
    "        return pos_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6594dbda",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "numerator is  1\ndenominator is 5\nnumerator is  1\ndenominator is 5\nnumerator is  1\ndenominator is 5\nnumerator is  1\ndenominator is 5\nnumerator is  1\ndenominator is 5\ndistribution is: [0.2, 0.2, 0.2, 0.2, 0.2]\nnumerator is  1\ndenominator is 10\nnumerator is  1\ndenominator is 10\nnumerator is  1\ndenominator is 10\nnumerator is  1\ndenominator is 10\nnumerator is  1\ndenominator is 10\ndistribution is: [0.1, 0.1, 0.1, 0.1, 0.1]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m dist \u001b[38;5;241m=\u001b[39m DirichletMultinominal()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dist\u001b[38;5;241m.\u001b[39mget_posterior_predictive() \u001b[38;5;241m==\u001b[39m [\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dist\u001b[38;5;241m.\u001b[39mget_posterior_predictive(observations\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m==\u001b[39m [\u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m]\n\u001b[1;32m      5\u001b[0m dist\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dist\u001b[38;5;241m.\u001b[39mget_posterior_predictive(observations\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m==\u001b[39m [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### TEST YOUR CODE\n",
    "dist = DirichletMultinominal()\n",
    "assert dist.get_posterior_predictive() == [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "assert dist.get_posterior_predictive(observations=[0, 0, 0, 0, 0]) == [0.6, 0.1, 0.1, 0.1, 0.1]\n",
    "dist.reset()\n",
    "assert dist.get_posterior_predictive(observations=[1, 1, 1, 1, 1]) == [0.1, 0.6, 0.1, 0.1, 0.1]\n",
    "print(\"Everything is correct!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f5c69cb",
   "metadata": {},
   "source": [
    "Run the last block to simulate our QMDP. Does the result make sense?\n",
    "\n",
    "Run it a few times, as well as changing the initial state so that you can see different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "state = [\"ego_1\", \"car_1\"]\n",
    "state_list = [state]\n",
    "b = dist.get_posterior_predictive()\n",
    "print(\"State: {}\\tAction: {}\\tBelief: {}\".format(state, None, b))\n",
    "\n",
    "while not done:\n",
    "    belief = propagate_belief(b, state[0])\n",
    "    next_action = QMDP(V_star=V_star, belief=belief, mdp=tinter_qmdp)\n",
    "    \n",
    "    if int(next_action) == 2:\n",
    "        true_state_idx = int(state[1].replace(\"car_\", \"\")) - 1\n",
    "        obs = np.random.choice([min(true_state_idx-1, 0), true_state_idx, min(true_state_idx+1, 4)], p=[0.1, 0.8, 0.1])\n",
    "        b = dist.get_posterior_predictive([obs])\n",
    "    elif int(next_action) == 0:\n",
    "        dist.reset()\n",
    "        b = dist.get_posterior_predictive()\n",
    "\n",
    "    state_index = tinter_qmdp.get_index(state)\n",
    "\n",
    "    next_state_indices = np.where(tinter_qmdp.P[:, state_index, int(next_action)] > 0.0)[0]\n",
    "    next_state_p = tinter_qmdp.P[:, state_index, int(next_action)][next_state_indices]\n",
    "    next_state_index = np.random.choice(next_state_indices, p=next_state_p)\n",
    "    \n",
    "    state = tinter_qmdp.get_real_state_value(next_state_index)\n",
    "    state_list.append(state)\n",
    "\n",
    "    if state[0] == \"ego_5\" or state[1] == \"car_5\":\n",
    "        done = True\n",
    "    \n",
    "    print(\"State: {}\\tAction: {}\\tBelief: {}\".format(state, next_action, b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}